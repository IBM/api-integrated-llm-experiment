{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import sys\n",
            "import os\n",
            "\n",
            "module_path = os.path.abspath(os.path.join(\"..\"))\n",
            "if module_path not in sys.path:\n",
            "    sys.path.append(module_path)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from api_integrated_llm.data_models.scorer_models import (\n",
            "    AggegatorOutputModel,\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from api_integrated_llm.helpers.file_helper import (\n",
            "    get_base_model_from_json,\n",
            "    get_files_in_folder,\n",
            ")\n",
            "\n",
            "bins_to_skip = set([0, 1, 6, 7, 8])\n",
            "\n",
            "model_list = []\n",
            "is_set = False\n",
            "is_micro = False\n",
            "output_file_name = (\n",
            "    \"gold_sequence_length_f1_by_model_slot_set\"\n",
            "    if is_set\n",
            "    else \"gold_sequence_length_f1_by_model_intent_list\"\n",
            ")\n",
            "\n",
            "model_names = set()\n",
            "\n",
            "\n",
            "model_small = set([\n",
            "    \"granite-3.1-8b-instruct\",\n",
            "    \"hammer-2.1-7b\",\n",
            "    \"llama-3.1-8b-instruct\",\n",
            "    \"mixtral-8x7b-instruct-v0.1\",\n",
            "    \"qwen-2.5-7b-instruct\",\n",
            "    \"watt-tool-8b\",\n",
            "])\n",
            "\n",
            "model_large = set([\n",
            "    \"deepseek-v3\",\n",
            "    \"gpt-4o\",\n",
            "    \"gpt-4o-tool-call\",\n",
            "    \"llama-3.3-70b-instruct\",\n",
            "    \"mixtral-8x22b-instruct-v0.1\",\n",
            "    \"qwen-2.5-72b-instruct\",\n",
            "])\n",
            "\n",
            "model_all = set([\n",
            "    \"deepseek-v3\",\n",
            "    \"gpt-4o\",\n",
            "    \"gpt-4o-tool-call\",\n",
            "    \"granite-3.1-8b-instruct\",\n",
            "    \"hammer-2.1-7b\",\n",
            "    \"llama-3.1-8b-instruct\",\n",
            "    \"llama-3.3-70b-instruct\",\n",
            "    \"mixtral-8x22b-instruct-v0.1\",\n",
            "    \"mixtral-8x7b-instruct-v0.1\",\n",
            "    \"qwen-2.5-72b-instruct\",\n",
            "    \"qwen-2.5-7b-instruct\",\n",
            "    \"watt-tool-8b\",\n",
            "])\n",
            "\n",
            "current_path = os.path.abspath(os.path.join(\".\"))\n",
            "file_paths = get_files_in_folder(\n",
            "    folder_path=os.path.join(current_path, \"data\", \"sequence_length\"),\n",
            "    file_extension=\"json\",\n",
            ")\n",
            "for file_path in file_paths:\n",
            "    obj = get_base_model_from_json(\n",
            "        file_path=file_path,\n",
            "        base_model=AggegatorOutputModel,\n",
            "    )\n",
            "    model_name = str(file_path).split(\"/\")[-1].split(\"_\")[-1][:-5]\n",
            "    \n",
            "    # if model_name not in model_small:\n",
            "    #     continue\n",
            "\n",
            "    target_metrics = (\n",
            "        obj.aggregated_metrics_problem_level[\"gold_output_length\"].slot_set_metrics\n",
            "        if is_set\n",
            "        else obj.aggregated_metrics_problem_level[\n",
            "            \"gold_output_length\"\n",
            "        ].intent_counter_metrics\n",
            "    )\n",
            "    data = []\n",
            "    target_list = target_metrics.micro if is_micro else target_metrics.macro\n",
            "    for k, metrics in target_list.items():\n",
            "        if int(k) in bins_to_skip:\n",
            "            continue\n",
            "        data.append((k, metrics.f1))\n",
            "    sorted_data = sorted(data, key=lambda datum: datum[0])\n",
            "\n",
            "    x = list(map(lambda unit: unit[0], sorted_data))\n",
            "    y = list(map(lambda unit: unit[1], sorted_data))\n",
            "    model_list.append((model_name, {\"x\": x, \"y\": y}))\n",
            "model_list = sorted(model_list, key=lambda unit: unit[0])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "\n",
            "fig, ax = plt.subplots()\n",
            "ax.set_prop_cycle(color=[\"blue\", \"orange\", \"green\", \"red\", \"purple\", \"brown\", \"teal\", \"orange\", \"yellow\", \"grey\", \"tan\", \"cyan\", \"aqua\", \"olive\"])\n",
            "current_label = \"\"\n",
            "current_legends = []\n",
            "for i, item in enumerate(model_list):\n",
            "    label = item[0]\n",
            "    content = item[1]\n",
            "    current_label = label\n",
            "    current_legends = content[\"x\"]\n",
            "    plt.plot(content[\"x\"], content[\"y\"], label=label)\n",
            "plt.xlabel(\"Gold sequence length\")\n",
            "plt.ylabel(\"Slot F1\" if is_set else \"Intent F1\")\n",
            "ax.set_xticks(current_legends)\n",
            "ax.set_xticklabels(current_legends)\n",
            "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
            "plt.grid()\n",
            "current_axes = plt.gca()\n",
            "current_axes.spines[\"top\"].set_visible(False)\n",
            "current_axes.spines[\"right\"].set_visible(False)\n",
            "plt.savefig(f\"{output_file_name}.svg\", bbox_inches=\"tight\")\n",
            "plt.savefig(f\"{output_file_name}.png\", bbox_inches=\"tight\")"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "ail",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.9"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
